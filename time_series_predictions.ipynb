{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os                    \n",
    "import matplotlib.pyplot as plt\n",
    "from fbprophet import Prophet\n",
    "import calendar\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data already treated in previous script is imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d_sales_full = pd.read_csv(\"./data/full_daily_series.csv\")\n",
    "df_all_sales = pd.read_csv(\"./data/df_all_monthly_sales.csv\")\n",
    "df_m_sales = pd.read_csv(\"./data/full_monthly_series.csv\")\n",
    "df_calendar = pd.read_csv(\"./data/calendar.csv\")\n",
    "df_working_dd = pd.read_csv(\"./data/working_days.csv\")\n",
    "df_actual_sales = pd.read_csv(\"./data/actual_sales_series.csv\")\n",
    "df_lo_sales = pd.read_csv(\"./data/lo_series.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual_sales = pd.read_csv(\"./data/actual_sales_series.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROPHET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the data to apply Prophet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pr = df_d_sales_full[[\"date\", \"daily_sales\"]]\n",
    "df_pr.columns = ['ds','y'] # redefine columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete df_pr in order to have all dates until end of month\n",
    "df_pr_addition_aux = {\n",
    "    'ds': ['2018-02-23', '2018-02-24', '2018-02-25', '2018-02-26', '2018-02-27', '2018-02-28'],\n",
    "    'y': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "}\n",
    "df_pr_addition = pd.DataFrame.from_dict(df_pr_addition_aux)\n",
    "df_pr = pd.concat([df_pr, df_pr_addition])\n",
    "df_pr = df_pr.reset_index()\n",
    "df_pr.drop('index', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SET PARAMETERS\n",
    "START_TRAINING_DATE = pd.to_datetime(df_d_sales_full.date.unique()[0])\n",
    "END_TRAINING_DATE = \"2017-01-01\"\n",
    "END_CV_DATE = \"2017-08-01\"\n",
    "END_TEST_DATE = \"2017-11-01\"\n",
    "END_HOLDOUT_DATE = \"2018-03-01\"\n",
    "FORECAST_HORIZON = 16 # for cv and for predicting in test and holdout to get full month forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define datasets and calculate percentages of train, cross validation and test.\n",
    "train_data = df_pr[df_pr.ds < END_TRAINING_DATE]\n",
    "cv_data = df_pr[(df_pr.ds >= END_TRAINING_DATE) & (df_pr.ds < END_CV_DATE)]\n",
    "test_data = df_pr[(df_pr.ds >= END_CV_DATE) & (df_pr.ds < END_TEST_DATE)]\n",
    "holdout = df_pr[df_pr.ds >= END_TEST_DATE]\n",
    "assert len(train_data) + len(cv_data) + len(test_data) + len(holdout) == len(df_pr)\n",
    "print(\"Number of data points in train, cv and test: {}, {},  {}\".format(len(train_data), \\\n",
    "                                                                       len(cv_data), len(test_data)))\n",
    "tot_ds = len(train_data) + len(cv_data) + len(test_data)\n",
    "print(\"Percentages of dataset (excluding holdout) distributed in training, cv, test: {}%, {}%, {}%\"\\\n",
    "              .format(round(len(train_data)/tot_ds*100,1), round(len(cv_data)/tot_ds*100,1), \\\n",
    "                      round(len(test_data)/tot_ds*100,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_format(string_date):\n",
    "    \"\"\"Converts a string to a datetime object.\"\"\"\n",
    "    dt_date = datetime.strptime(string_date, '%Y-%m-%d')\n",
    "    return dt_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select number of days for training: it will always be the same throughout cross-validation.\n",
    "first_day_cv = datetime_format(cv_data.ds.astype(str).unique()[0])\n",
    "train_length = (first_day_cv - START_TRAINING_DATE).days\n",
    "train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ymd(string_date,i):\n",
    "    \"\"\"Obtains year (i=0), month (i=1) or day (i=2)\n",
    "    from a string date.\"\"\"\n",
    "    if i in [0,1,2]:\n",
    "        value = int(string_date.split('-')[i])\n",
    "    else:\n",
    "        print(\"Parameter 'i' should be 0 for year, 1 for month, 2 for day.\")\n",
    "    return value\n",
    "\n",
    "def get_cv_dates_by_yymm(start_date, end_date):\n",
    "    \"\"\"Given a start and an end date, calculates all dates inbetween\n",
    "    and appends them to a list as strings.\"\"\"\n",
    "    sdy = get_ymd(start_date,0)\n",
    "    sdm = get_ymd(start_date,1)\n",
    "    sdd = get_ymd(start_date,2)\n",
    "    sdate = date(sdy, sdm, sdd)\n",
    "    \n",
    "    edy = get_ymd(end_date,0)\n",
    "    edm = get_ymd(end_date,1)\n",
    "    edd = get_ymd(end_date,2)\n",
    "    edate = date(edy, edm, edd)\n",
    "    \n",
    "    delta = edate - sdate\n",
    "    all_days = []\n",
    "    for i in range(delta.days + 1):\n",
    "        day = sdate + timedelta(days=i)\n",
    "        if day.day in range(15,26):\n",
    "            all_days.append(str(day))\n",
    "    return all_days\n",
    "\n",
    "# Calculate training dates for cross-validation.\n",
    "cv_dates = []\n",
    "cv_dates = get_cv_dates_by_yymm(END_TRAINING_DATE, END_CV_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CV dates are not holidays: if so, remove them\n",
    "holidays = df_calendar[df_calendar.holiday == 0].date.astype(str).unique()\n",
    "cv_dates_final = [x for x in cv_dates if x not in holidays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove weekends from holidays dataset to add regressor\n",
    "df_calendar_holidays = df_calendar[(df_calendar.holiday == 0) & \\\n",
    "                                  (df_calendar.weekday != \"Saturday\") &\\\n",
    "                                  (df_calendar.weekday !=\"Sunday\")].date.astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSORS\n",
    "\n",
    "# Holidays\n",
    "holidays_regr = pd.DataFrame({\n",
    "  'holiday': 'country_1_holidays',\n",
    "  'ds': pd.to_datetime(list(df_calendar_holidays)),\n",
    "  'lower_window': 0,\n",
    "  'upper_window': 0,\n",
    "})\n",
    "\n",
    "# Days of week with higher sales\n",
    "def get_higher_sales_days(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    if date.weekday() in [0,1,2]: #Mon, Tue, Wed\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Month of the year\n",
    "def get_month_of_year(ds,m):\n",
    "    date = pd.to_datetime(ds)\n",
    "    if date.month == m:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Week of the month\n",
    "#def get_week_of_month(df):\n",
    "\n",
    "# LO of the month --> not applied\n",
    "def add_LO(df, df_LO):\n",
    "    df['ds'] = df['ds'].apply(datetime_format)\n",
    "    df['yymm'] = df.ds.map(lambda x: x.strftime('%Y-%m'))\n",
    "    df_LO.columns = [\"LO_monthly_sales\", \"yymm\"]\n",
    "    df_tot = df.merge(df_LO, on=\"yymm\", how=\"left\")\n",
    "    df_tot.drop('yymm', axis=1, inplace=True)\n",
    "    df_tot['ds'] = df_tot.ds.astype(str)\n",
    "    return df_tot\n",
    "    \n",
    "df_pr['is_higher_sales_day'] = df_pr.ds.apply(get_higher_sales_days)\n",
    "df_pr['is_jan'] = df_pr.ds.apply(get_month_of_year, m=1)\n",
    "df_pr['is_feb'] = df_pr.ds.apply(get_month_of_year, m=2)\n",
    "df_pr['is_mar'] = df_pr.ds.apply(get_month_of_year, m=3)\n",
    "df_pr['is_apr'] = df_pr.ds.apply(get_month_of_year, m=4)\n",
    "df_pr['is_may'] = df_pr.ds.apply(get_month_of_year, m=5)\n",
    "df_pr['is_jun'] = df_pr.ds.apply(get_month_of_year, m=6)\n",
    "df_pr['is_jul'] = df_pr.ds.apply(get_month_of_year, m=7)\n",
    "df_pr['is_aug'] = df_pr.ds.apply(get_month_of_year, m=8)\n",
    "df_pr['is_sep'] = df_pr.ds.apply(get_month_of_year, m=9)\n",
    "df_pr['is_oct'] = df_pr.ds.apply(get_month_of_year, m=10)\n",
    "df_pr['is_nov'] = df_pr.ds.apply(get_month_of_year, m=11)\n",
    "df_pr['is_dec'] = df_pr.ds.apply(get_month_of_year, m=12)\n",
    "#df_pr = add_LO(df_pr, df_lo_sales)\n",
    "\n",
    "df_pr.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PARAMETER GRID: \n",
    "# We add here the prior scale and the fourier mode in order to tweak width and height of seasonality bumps\n",
    "parameter_grid = {\n",
    "    'prior_scale_regressor': [0.5, 1],\n",
    "    'fourier_order': [5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to add regressors to prophet model and to future predictions\n",
    "def get_model(fourier_mode, prior_scale):\n",
    "    \"\"\"Sets the model according to regressors, seasonalitiers \n",
    "    and holidays chosen. Outputs the model.\"\"\"\n",
    "    \n",
    "    m = Prophet(seasonality_mode = 'additive', weekly_seasonality= False,  daily_seasonality = False, \n",
    "                   yearly_seasonality = False, holidays=holidays_regr)\n",
    "    m.add_seasonality(period=7, name='week', fourier_order = fourier_mode, mode = 'additive')\n",
    "    m.add_seasonality(period=30.5, name='month', fourier_order = fourier_mode, mode = 'additive')\n",
    "    m.add_seasonality(period=365.25, name='year', fourier_order = fourier_mode, mode = 'additive')\n",
    "    m.add_regressor('is_higher_sales_day',prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_jan', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_feb', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_mar', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_apr', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_may', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_jun', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_jul', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_aug', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_sep', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_oct', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_nov', prior_scale=prior_scale, mode='additive')\n",
    "    m.add_regressor('is_dec', prior_scale=prior_scale, mode='additive')\n",
    "    #m.add_regressor('LO_monthly_sales', prior_scale=prior_scale, mode='additive')\n",
    "    return m\n",
    "\n",
    "def get_future_preds(m, df, forecast_horizon, month_cv_date=None):\n",
    "    \"\"\"Given a model (m) and a forecast horizon, generates future dataset\n",
    "    and predictions. The original df is given in order to filter our any future\n",
    "    date that Prophet includes (by setting business days for example), but\n",
    "    that would not be included in the original dataset.\n",
    "    month_cv_date is used to further filter future predictions to the month \n",
    "    of the cutoff.\"\"\"\n",
    "    \n",
    "    future_aux = m.make_future_dataframe(periods = forecast_horizon, freq = \"B\") #freq = \"B\" for business days\n",
    "    future_aux = future_aux[future_aux.ds.isin(df.ds.unique())] # filter out any other day which is not in original series, just in case\n",
    "    future_aux[\"month\"] = pd.to_datetime(future_aux.ds).apply(lambda d: d.month) # keep only dates within same month of cv_date\n",
    "    \n",
    "    if month_cv_date:\n",
    "        future = future_aux[future_aux.month == month_cv_date][['ds']]\n",
    "    else:\n",
    "        future = future_aux\n",
    "        \n",
    "    future['is_higher_sales_day'] = df_pr.ds.apply(get_higher_sales_days)\n",
    "    future['is_jan'] = future.ds.apply(get_month_of_year, m=1)\n",
    "    future['is_feb'] = future.ds.apply(get_month_of_year, m=2)\n",
    "    future['is_mar'] = future.ds.apply(get_month_of_year, m=3)\n",
    "    future['is_apr'] = future.ds.apply(get_month_of_year, m=4)\n",
    "    future['is_may'] = future.ds.apply(get_month_of_year, m=5)\n",
    "    future['is_jun'] = future.ds.apply(get_month_of_year, m=6)\n",
    "    future['is_jul'] = future.ds.apply(get_month_of_year, m=7)\n",
    "    future['is_aug'] = future.ds.apply(get_month_of_year, m=8)\n",
    "    future['is_sep'] = future.ds.apply(get_month_of_year, m=9)\n",
    "    future['is_oct'] = future.ds.apply(get_month_of_year, m=10)\n",
    "    future['is_nov'] = future.ds.apply(get_month_of_year, m=11)\n",
    "    future['is_dec'] = future.ds.apply(get_month_of_year, m=12)\n",
    "    return future\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ad-hoc cross-validation function. \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "def cross_val_fct(df, cv_dates, train_length, forecast_horizon, pred_type): \n",
    "    \"\"\"Takes in list of dates (cv_dates), performs for loop where\n",
    "    for each one with the decided number of training days (train_length) is used\n",
    "    to forecasts a number of forecast_horizon dates.\n",
    "    pred_type can be 'cv' or 'test' (used to filter out predictions before the first prediction \n",
    "    date, which is not done for test sets).\n",
    "    The loop aggregates the prediction results into a dataframe (containing cutoff date, prediction dates\n",
    "    for each cutoff date, and the prediction results. This dataframe is the output.\n",
    "    Notice the cutoff is not included in the prediction, but rather in the training.\"\"\"\n",
    "    \n",
    "    results_cv = pd.DataFrame()\n",
    "    for i in cv_dates:\n",
    "        # Set always only the same number of days for training:\n",
    "        cutoff = str((datetime_format(i) - timedelta(days=1)).date()) # say 2017-01-14\n",
    "        first_pred_day = i # 2017-01-15\n",
    "        first_training_date = str((datetime_format(first_pred_day) - timedelta(days=train_length)).date())\n",
    "        \n",
    "        # Set train and test sets within cross validation\n",
    "        print(\"For cross-validation starting on {}, training starts on {}\".format(first_pred_day, first_training_date))\n",
    "        cv_train_data = df[(df.ds < first_pred_day) & (df.ds >= first_training_date)]\n",
    "        cv_test_data = df[df.ds >= first_pred_day]\n",
    "        print(\"Predicting for day {}\".format(first_pred_day))\n",
    "        \n",
    "        # Train models (we fix here the prior scale and the fourier mode, but these can be turned into parameters\n",
    "        # for the parameter grid).\n",
    "        prior_scale = 0.5\n",
    "        fourier_mode = 5\n",
    "        m = get_model(fourier_mode, prior_scale)\n",
    "        m.fit(cv_train_data)\n",
    "        \n",
    "        # Make future predictions\n",
    "        month_cv_date = int(i.split('-')[1])\n",
    "        future = get_future_preds(m, df, forecast_horizon, month_cv_date)\n",
    "        \n",
    "        # Predicting\n",
    "        print(\"Predicting..\")\n",
    "        forecast = m.predict(future)\n",
    "        #fig = m.plot_components(forecast) # plot if wanted\n",
    "        if pred_type == \"cv\":\n",
    "            to_add = forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]][(forecast.ds >= first_pred_day)]\n",
    "        elif pred_type == \"test\":\n",
    "            to_add = forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "        else:\n",
    "            print(\"Parameter 'pred_type' must be either 'cv' or 'test'.\")\n",
    "        to_add['cutoff'] = cutoff\n",
    "        \n",
    "        # concatenate results based on horizon\n",
    "        results_cv = pd.concat([results_cv, to_add])\n",
    "    return results_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasts for cross validation:\n",
    "fcst = cross_val_fct(df_pr, cv_dates_final, train_length, FORECAST_HORIZON, 'cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the information about the horizon days\n",
    "fcst.cutoff = pd.to_datetime(fcst.cutoff, format='%Y-%m-%d')\n",
    "df_pr.ds = pd.to_datetime(df_pr.ds, format='%Y-%m-%d')\n",
    "fcst = fcst.merge(df_pr, on=\"ds\", how=\"left\")\n",
    "fcst[\"horizon\"] = fcst[\"ds\"] - fcst[\"cutoff\"]\n",
    "fcst[\"horizon\"] = fcst[\"horizon\"].apply(lambda d: d.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst[fcst.cutoff == \"2017-01-15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Prophet metrics to calculate rolling mae and mse as a function of the horizon.\n",
    "from fbprophet.diagnostics import performance_metrics\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "performance_metrics(fcst, rolling_window=4./ fcst.shape[0], metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "plot_cross_validation_metric(fcst, metric = 'mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT IN TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_dates(start_date, end_date, holidays):\n",
    "    \"\"\"Given start date and an end date, generate all \n",
    "    the dates in between. Then filters out holidays.\"\"\"\n",
    "    \n",
    "    prediction_dates = []\n",
    "    prediction_dates = get_cv_dates_by_yymm(start_date, end_date)\n",
    "    prediction_dates_final = [x for x in prediction_dates if x not in holidays]\n",
    "    return prediction_dates_final\n",
    "\n",
    "def get_n_training_dd(prediction_dates_final):\n",
    "    \"\"\" Outputs number of training dates give the prediction dates.\"\"\"\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    first_day_test = datetime_format(min(prediction_dates_final))\n",
    "    train_length = (first_day_test - START_TRAINING_DATE).days\n",
    "    return train_length\n",
    "\n",
    "def get_predictions(df, pred_dates, forecast_horizon): \n",
    "    \"\"\"Outputs all models, future datasets and prediction results datasets\n",
    "    obtained for forecasting a forecast_horizon number of days for each day in pred_dates.\"\"\"\n",
    "    all_models = {}\n",
    "    all_futures = {}\n",
    "    results_pred = pd.DataFrame()\n",
    "    for i in pred_dates:\n",
    "        # Set always only the same number of days for training:\n",
    "        cutoff = str((datetime_format(i) - timedelta(days=1)).date()) # say 2017-01-14\n",
    "        first_pred_day = i # 2017-01-15\n",
    "        first_training_date = START_TRAINING_DATE # always keep the beginning to get maximum out of training\n",
    "        \n",
    "        # Set train and test sets \n",
    "        train_data = df[(df.ds < first_pred_day) & (df.ds >= first_training_date)]\n",
    "        test_data = df[df.ds >= first_pred_day]\n",
    "        print(\"Predicting for day {} using all training set available\".format(first_pred_day))\n",
    "        \n",
    "        # Train models # comentar\n",
    "        prior_scale = 0.5\n",
    "        fourier_mode = 5\n",
    "        m = get_model(fourier_mode, prior_scale)\n",
    "        m.fit(train_data)\n",
    "        \n",
    "        # Make future predictions\n",
    "        future = get_future_preds(m, df, forecast_horizon)\n",
    "        \n",
    "        # Predicting\n",
    "        print(\"Predicting..\")\n",
    "        forecast = m.predict(future)\n",
    "        #fig = m.plot_components(forecast) # plot if wanted\n",
    "        to_add = forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "        to_add['cutoff'] = cutoff\n",
    "        \n",
    "        # Concatenate results based on horizon, concatenate also all models and future datasets (one of both for each predicted date)\n",
    "        results_pred = pd.concat([results_pred, to_add])\n",
    "        all_models[first_pred_day] = m\n",
    "        all_futures[first_pred_day] = future\n",
    "    return all_models, all_futures, results_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_test_date(test_dates_final):\n",
    "    \"\"\" Returns the first day of the month of\n",
    "    test_dates_final.\"\"\"\n",
    "    \n",
    "    ref_date = min(test_dates_final)\n",
    "    min_test_forecast_date = ref_date[0:8] + \"01\"\n",
    "    return min_test_forecast_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all predictiond dates\n",
    "test_dates_final = get_prediction_dates(END_CV_DATE, END_TEST_DATE, holidays)\n",
    "df_pr.ds = pd.to_datetime(df_pr.ds, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for test set\n",
    "test_m, test_future, test_forecast = get_predictions(df_pr, test_dates_final, FORECAST_HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test forecasts to original dataset to get real daily value.\n",
    "test_forecast.cutoff = pd.to_datetime(test_forecast.cutoff, format='%Y-%m-%d')\n",
    "#df_pr.ds = pd.to_datetime(df_pr.ds, format='%Y-%m-%d')\n",
    "test_forecast = test_forecast.merge(df_pr, on=\"ds\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset to get only results starting at the beginning of the month of the test set. \n",
    "# This is needed to calculate full month forecast.\n",
    "min_test_forecast_date = get_min_test_date(test_dates_final)\n",
    "test_forecast_cut = test_forecast[(test_forecast.ds >= min_test_forecast_date)]\n",
    "test_forecast_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add necessary columns used to filter results (specifically, results only for the month of the cutoff date, \n",
    "# otherwise we would be adding up values from other months).\n",
    "test_forecast_cut['month_cutoff'] = test_forecast_cut['cutoff'].apply(lambda x: x.month)\n",
    "test_forecast_cut['year_cutoff'] = test_forecast_cut['cutoff'].apply(lambda x: x.year)\n",
    "test_forecast_cut['month_ds'] = test_forecast_cut['ds'].apply(lambda x: x.month)\n",
    "test_forecast_cut['year_ds'] = test_forecast_cut['ds'].apply(lambda x: x.year)\n",
    "test_forecast_cut = test_forecast_cut[(test_forecast_cut.month_cutoff == test_forecast_cut.month_ds) & \\\n",
    "                                           (test_forecast_cut.year_cutoff == test_forecast_cut.year_ds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Get predictions using samples to estimate errors for aggregate forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we want the full-month prediction on the prediction date 2017-11-16. For this date the next 16 days will be forecasted. For each of these days, we can perform predictive samples, which are 1000 sample predictions for that prediction date (instead of only one forecast).\n",
    "\n",
    "The procedure is: \n",
    "- Take all the predicted values for the prediction dates within the month (in this case November), and aggregate them (sum) —> now we have 1000 aggregated values (a distribution of the aggregated values). \n",
    "- Of these 1000 aggregations, the mean is taken to consider the prediction value, and the 10th percentile and 90th percentiles are taken to consider the upper and lower values (80% uncertainty boundary). \n",
    "\n",
    "For each prediction date, aggregate over the remaining portion of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_forecast_error(m, future, pred_dates):\n",
    "    \"\"\" Obtains predictive samples. Takes in the model, the future dataset \n",
    "    and a list prediction dates to loop over.\"\"\"\n",
    "    \n",
    "    all_predictions_errors = pd.DataFrame()\n",
    "    for i in pred_dates:\n",
    "        #  Create samples of predictions for every specific day in future\n",
    "        samples = m[i].predictive_samples(future[i])\n",
    "\n",
    "        # Creates a DF where each column is 1 prediction for that specific day\n",
    "        samples_df = pd.DataFrame.from_records(samples[\"yhat\"])\n",
    "        samples_df['date'] = future[i]['ds']\n",
    "        samples_df['month'] = samples_df['date'].apply(lambda x: x.month)\n",
    "        month_min_fcst_date = int(i.split('-')[1])\n",
    "        # \n",
    "        samples_df = samples_df[(samples_df.date >= i) & (samples_df.month == month_min_fcst_date)]   \n",
    "        \n",
    "        # The mean of each column is thus our yhat\n",
    "        rest_of_month_predict = samples_df.groupby(\"month\").sum().mean(axis=1)\n",
    "        rest_of_month_predict = rest_of_month_predict.reset_index()\n",
    "        rest_of_month_predict['prediction_day'] = i\n",
    "        rest_of_month_predict.rename(columns={0: \"yhat\"}, inplace=True)\n",
    "        rest_of_month_predict = rest_of_month_predict[['prediction_day', 'month', 'yhat']]\n",
    "        #rest_of_month_predict['start_of_week'] = samples_df.groupby(\"week_of_year\").date.min().reset_index().date.tolist()\n",
    "\n",
    "        # Upper and lower values of yhat are computed following fbprophet's approach\n",
    "        upper_lower = samples_df.groupby(\"month\").sum().reset_index()\n",
    "        rest_of_month_predict['yhat_lower'] = upper_lower.apply(lambda x: np.percentile(x, 10), axis=1).tolist()\n",
    "        rest_of_month_predict['yhat_upper'] = upper_lower.apply(lambda x: np.percentile(x, 90), axis=1).tolist()\n",
    "        \n",
    "        all_predictions_errors = pd.concat([all_predictions_errors, rest_of_month_predict])\n",
    "    return all_predictions_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rolled_preds = get_aggregate_forecast_error(test_m, test_future, test_dates_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rolled_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates real values up to the cutoff (included):\n",
    "all_real = test_forecast_cut[test_forecast_cut.ds <= test_forecast_cut.cutoff][['ds', 'y', 'cutoff']]\n",
    "all_real = test_forecast_cut[(test_forecast_cut.ds <= test_forecast_cut.cutoff) & \\\n",
    "                               ((test_forecast_cut.month_cutoff == test_forecast_cut.month_ds) & \\\n",
    "                                           (test_forecast_cut.year_cutoff == test_forecast_cut.year_ds))][['ds', 'y', 'cutoff']]\n",
    "all_real['prediction_day'] = all_real['cutoff'].apply(lambda x: (x + timedelta(days=1)).date())\n",
    "all_rolled_real = all_real.groupby('prediction_day').sum().reset_index()\n",
    "all_rolled_real['prediction_day'] = all_rolled_real.prediction_day.astype(str)\n",
    "all_rolled_real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge rolled predictions with rolled real values to obtain the monthly forecast for each day.\n",
    "test_aggr_forecasts = all_rolled_preds.merge(all_rolled_real, on=\"prediction_day\", how='outer')\n",
    "test_aggr_forecasts['monthly_forecast'] = test_aggr_forecasts['y'] + test_aggr_forecasts['yhat']\n",
    "test_aggr_forecasts = test_aggr_forecasts[['prediction_day', 'monthly_forecast', 'yhat_lower', 'yhat_upper']]\n",
    "test_aggr_forecasts.columns = ['prediction_day', 'monthly_forecast', 'monthly_forecast_lower', 'monthly_forecast_upper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aggr_forecasts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fcst_agg_actual_results(test_aggr_forecasts,df_actual_sales):\n",
    "    \"\"\"Calculates custom variance (percentage difference between \n",
    "    forecasted and real value).\"\"\"\n",
    "    \n",
    "    test_aggr_forecasts.prediction_day = pd.to_datetime(test_aggr_forecasts.prediction_day, format='%Y-%m-%d')\n",
    "    test_aggr_forecasts['month'] = test_aggr_forecasts['prediction_day'].apply(lambda x: x.month)\n",
    "    test_aggr_forecasts['year'] = test_aggr_forecasts['prediction_day'].apply(lambda x: x.year)\n",
    "    test_aggr_forecasts['yymm'] = 100*test_aggr_forecasts.year + test_aggr_forecasts.month\n",
    "    test_aggr_forecasts['yymm'] = pd.to_datetime(test_aggr_forecasts.yymm, format='%Y%m').map(lambda x: x.strftime('%Y-%m'))\n",
    "    \n",
    "    df_actual_sales.yymm = pd.to_datetime(df_actual_sales.yymm, format='%Y-%m').map(lambda x: x.strftime('%Y-%m'))\n",
    "    test_forecast_comparison = test_aggr_forecasts.merge(df_actual_sales, on=\"yymm\", how='left')\n",
    "    test_forecast_comparison[\"delta\"] = test_forecast_comparison.monthly_forecast - test_forecast_comparison.monthly_sales\n",
    "    test_forecast_comparison[\"variance\"] = test_forecast_comparison.delta/test_forecast_comparison.monthly_sales\n",
    "    return test_forecast_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecast_comparison = get_fcst_agg_actual_results(test_aggr_forecasts,df_actual_sales)\n",
    "test_forecast_comparison.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecast_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot custom variance results for test set (where we actually have actual results)\n",
    "test_forecast_comparison['dd'] = test_forecast_comparison.prediction_day.apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(test_forecast_comparison[test_forecast_comparison.yymm == '2017-08'][\"dd\"], test_forecast_comparison[test_forecast_comparison.yymm == '2017-08'][\"variance\"], linestyle=\"-\", color='#346BAE')\n",
    "plt.plot(test_forecast_comparison[test_forecast_comparison.yymm == '2017-09'][\"dd\"], test_forecast_comparison[test_forecast_comparison.yymm == '2017-09'][\"variance\"], linestyle=\"-\", color='#D55632')\n",
    "plt.plot(test_forecast_comparison[test_forecast_comparison.yymm == '2017-10'][\"dd\"], test_forecast_comparison[test_forecast_comparison.yymm == '2017-10'][\"variance\"], linestyle=\"-\", color='#E09E3F')\n",
    "plt.axhline(y=0.0, color='#d0d0d0', linestyle='--')\n",
    "plt.xlabel=('Dates')\n",
    "plt.ylabel=('Custom Variance')\n",
    "plt.legend([\"2017-08\", \"2017-09\", \"2017-10\"])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Get predictions as simple sum of prophet forecasts (no error estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_pred_by_cutoff(test_forecast_cut):\n",
    "    #fcst.cutoff = pd.to_datetime(fcst.cutoff, format='%Y-%m-%d')\n",
    "    test_forecast_cut['y_to_accum'] = np.where(test_forecast_cut.ds <= test_forecast_cut.cutoff, \\\n",
    "                                               test_forecast_cut.y, test_forecast_cut.yhat)\n",
    "    test_forecast_cut['y_to_accum_upper'] = np.where(test_forecast_cut.ds <= test_forecast_cut.cutoff, \\\n",
    "                                               test_forecast_cut.y, test_forecast_cut.yhat)\n",
    "    test_forecast_cut['month'] = test_forecast_cut['ds'].apply(lambda x: x.month)\n",
    "    test_forecast_cut['year'] = test_forecast_cut['ds'].apply(lambda x: x.year)\n",
    "    test_forecast_cut['yymm'] = 100*test_forecast_cut.year + test_forecast_cut.month\n",
    "    test_forecast_cut['yymm'] = pd.to_datetime(test_forecast_cut.yymm, format='%Y%m').map(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "    test_forecast_cut_grpd = test_forecast_cut.groupby([\"cutoff\",\"yymm\"]).agg({'y_to_accum': ['sum']})\n",
    "    test_forecast_cut_grpd.columns = ['_'.join(col) for col in test_forecast_cut_grpd.columns]\n",
    "    test_forecast_cut_grpd = test_forecast_cut_grpd.reset_index()\n",
    "    return test_forecast_cut_grpd\n",
    "        \n",
    "def get_fcst_agg_actual_results(test_forecast_cut_grpd,df_actual_sales):\n",
    "    test_forecast_comparison = test_forecast_cut_grpd.merge(df_actual_sales, on=\"yymm\", how='left')\n",
    "    test_forecast_comparison[\"delta\"] = test_forecast_comparison.y_to_accum_sum - test_forecast_comparison.monthly_sales\n",
    "    test_forecast_comparison[\"variance\"] = test_forecast_comparison.delta/test_forecast_comparison.monthly_sales\n",
    "    return test_forecast_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecast_cut_grpd = get_agg_pred_by_cutoff(test_forecast_cut)\n",
    "test_forecast_comparison_2 = get_fcst_agg_actual_results(test_forecast_cut_grpd,df_actual_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecast_comparison_2['dd'] = test_forecast_comparison_2.cutoff.apply(lambda x: x.day)\n",
    "test_forecast_comparison_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot custom variance: we see it doesn't change compared to the previous.\n",
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(test_forecast_comparison_2[test_forecast_comparison_2.yymm == '2017-08'][\"dd\"], test_forecast_comparison_2[test_forecast_comparison_2.yymm == '2017-08'][\"variance\"], linestyle=\"-\", color='#346BAE')\n",
    "plt.plot(test_forecast_comparison_2[test_forecast_comparison_2.yymm == '2017-09'][\"dd\"], test_forecast_comparison_2[test_forecast_comparison_2.yymm == '2017-09'][\"variance\"], linestyle=\"-\", color='#D55632')\n",
    "plt.plot(test_forecast_comparison_2[test_forecast_comparison_2.yymm == '2017-10'][\"dd\"], test_forecast_comparison_2[test_forecast_comparison_2.yymm == '2017-10'][\"variance\"], linestyle=\"-\", color='#E09E3F')\n",
    "plt.axhline(y=0.0, color='#d0d0d0', linestyle='--')\n",
    "plt.xlabel=('Dates')\n",
    "plt.ylabel=('Custom Variance')\n",
    "plt.legend([\"2017-08\", \"2017-09\", \"2017-10\"])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test set we also compare to a baseline (t0 or naïve) model. The baseline model simply takes the value of the previous day. Here we take all the training and cross validation sets to train, and we predict on the horizon of all test set.\n",
    "We then calculate MAE and MSE of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline horizon (all test set)\n",
    "baseline_date = min(test_data.ds)\n",
    "from datetime import datetime, timedelta\n",
    "first_day_baseline = datetime_format(baseline_date)\n",
    "train_length = (first_day_baseline - START_TRAINING_DATE).days\n",
    "train_length #1048\n",
    "\n",
    "baseline_horizon = (datetime_format(END_TEST_DATE) - first_day_baseline).days\n",
    "print(first_day_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data\n",
    "#baseline_forecast = cross_val_fct(df_pr, baseline_date, train_length, FORECAST_HORIZON, 'test')\n",
    "baseline_train_data = df_pr[(df_pr.ds < first_day_baseline)]\n",
    "baseline_test_data = df_pr[df_pr.ds >= first_day_baseline]\n",
    "m = get_model(5, 0.5)\n",
    "m.fit(baseline_train_data)\n",
    "future = get_future_preds(m, df_pr, baseline_horizon)\n",
    "baseline_pred = m.predict(future)\n",
    "m.plot(baseline_pred)\n",
    "m.plot_components(baseline_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define errors\n",
    "baseline_pred_short = baseline_pred.copy()\n",
    "baseline_pred_short = baseline_pred_short.merge(df_pr, on=\"ds\", how=\"left\")[[\"ds\", \"yhat\",\"y\"]]\n",
    "baseline_pred_short[\"ybase\"] = baseline_pred_short.y.shift(1) # We imagine to make as a prediction the value of the day before\n",
    "baseline_pred_short[\"ae_prophet\"] = abs(baseline_pred_short.yhat - baseline_pred_short.y)\n",
    "baseline_pred_short[\"ae_baseline\"] = abs(baseline_pred_short.ybase - baseline_pred_short.y)\n",
    "baseline_pred_short[\"se_prophet\"] = (baseline_pred_short.yhat - baseline_pred_short.y)**2\n",
    "baseline_pred_short[\"se_baseline\"] = (baseline_pred_short.ybase - baseline_pred_short.y)**2\n",
    "baseline_pred_short = baseline_pred_short[baseline_pred_short.ds >= \"2015-01-02\"]\n",
    "\n",
    "# ERRORS\n",
    "mae_train_prophet = round(baseline_pred_short[baseline_pred_short.ds < first_day_baseline].ae_prophet.mean(),2)\n",
    "mae_train_baseline = round(baseline_pred_short[baseline_pred_short.ds < first_day_baseline].ae_baseline.mean(),2)\n",
    "mae_test_prophet = round(baseline_pred_short[baseline_pred_short.ds >= first_day_baseline].ae_prophet.mean(),2)\n",
    "mae_test_baseline = round(baseline_pred_short[baseline_pred_short.ds >= first_day_baseline].ae_baseline.mean(),2)\n",
    "\n",
    "mse_train_prophet = round(baseline_pred_short[baseline_pred_short.ds < first_day_baseline].se_prophet.mean(),2)\n",
    "mse_train_baseline = round(baseline_pred_short[baseline_pred_short.ds < first_day_baseline].se_baseline.mean(),2)\n",
    "mse_test_prophet = round(baseline_pred_short[baseline_pred_short.ds >= first_day_baseline].se_prophet.mean(),2)\n",
    "mse_test_baseline = round(baseline_pred_short[baseline_pred_short.ds >= first_day_baseline].se_baseline.mean(),2)\n",
    "\n",
    "rmse_train_prophet = round(math.sqrt(mse_train_prophet),2)\n",
    "rmse_train_baseline = round(math.sqrt(mse_train_baseline),2)\n",
    "rmse_test_prophet = round(math.sqrt(mse_test_prophet),2)\n",
    "rmse_test_baseline = round(math.sqrt(mse_test_baseline),2)\n",
    "\n",
    "print(mae_train_prophet, mae_train_baseline)\n",
    "print(mae_test_prophet, mae_test_baseline)\n",
    "print(mse_train_prophet, mse_train_baseline)\n",
    "print(mse_test_prophet, mse_test_baseline)  \n",
    "print(rmse_train_prophet, rmse_train_baseline)\n",
    "print(rmse_test_prophet, rmse_test_baseline)  # OK, seems baseline errors are much bigger!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for errors in order to plot them\n",
    "errors_baseline_dix_1 = {\n",
    "    'dataset': ['train', 'test'], \n",
    "    'MAE_Prophet': [mae_train_prophet, mae_test_prophet],\n",
    "    'MAE_Baseline': [mae_train_baseline, mae_test_baseline],\n",
    "    'MSE_Prophet': [mse_train_prophet, mse_test_prophet],\n",
    "    'MSE_Baseline': [mse_train_baseline, mse_test_baseline]\n",
    "}\n",
    "errors_baseline_dix = {\n",
    "    'metric': ['MAE_Prophet', \"MAE_Baseline\", \"MSE_Prophet\", \"MSE_Baseline\"],\n",
    "    'train': [mae_train_prophet, mae_train_baseline, mse_train_prophet, mse_train_baseline], \n",
    "    'test': [mae_test_prophet, mae_test_baseline, mse_test_prophet, mse_test_baseline]\n",
    "}\n",
    "errors_baseline_df = pd.DataFrame.from_dict(errors_baseline_dix)\n",
    "#errors_baseline_df.train = errors_baseline_df.train.apply(lambda x: np.log(x))\n",
    "#errors_baseline_df.test = errors_baseline_df.test.apply(lambda x: np.log(x))\n",
    "errors_baseline_df = errors_baseline_df.set_index(\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot\n",
    "print(errors_baseline_df)\n",
    "ax = errors_baseline_df[errors_baseline_df.index.isin([\"MAE_Prophet\", \"MAE_Baseline\"])].plot.bar(rot=0, color=[\"#346BAE\", \"#E09E3F\"],figsize=(10,6))\n",
    "ax = errors_baseline_df[errors_baseline_df.index.isin([\"MSE_Prophet\", \"MSE_Baseline\"])].plot.bar(rot=0, color=[\"#346BAE\", \"#E09E3F\"],figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICT IN HOLDOUT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict for the holdout, in the same way we did for the test set (we will use method 1 in test set where we can calculate the uncertainty boundaries for the aggregates as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_addition_aux = {\n",
    "    'date': ['2018-02-24', '2018-02-25', '2018-02-26', '2018-02-27', '2018-02-28'],\n",
    "    'yymm': ['2018-02', '2018-02', '2018-02', '2018-02', '2018-02'],\n",
    "    'weekday': ['Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday'],\n",
    "    'year': [2018, 2018, 2018, 2018, 2018],\n",
    "    'month': [2,2,2,2,2],\n",
    "    'month_lit': ['Feb', 'Feb', 'Feb', 'Feb', 'Feb'],\n",
    "    'day': [24, 25, 26, 27, 28],\n",
    "    'holiday': [0,0,1,1,1],\n",
    "    'is_real_day_sale': [0,0,0,0,1]\n",
    "} # we had erased these dates but we need them, so we add them here.\n",
    "df_calendar_addition = pd.DataFrame.from_dict(df_calendar_addition_aux)\n",
    "df_calendar_complete = pd.concat([df_calendar, df_calendar_addition])\n",
    "holidays = df_calendar_complete[df_calendar_complete.holiday == 0].date.astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all holdout prediction dates\n",
    "holdout_dates_final = get_prediction_dates(END_TEST_DATE, END_HOLDOUT_DATE, holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "ho_m, ho_future, holdout_forecast = get_predictions(df_pr, holdout_dates_final, FORECAST_HORIZON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to get real values as well\n",
    "holdout_forecast.cutoff = pd.to_datetime(holdout_forecast.cutoff, format='%Y-%m-%d')\n",
    "df_pr.ds = pd.to_datetime(df_pr.ds, format='%Y-%m-%d')\n",
    "holdout_forecast = holdout_forecast.merge(df_pr, on=\"ds\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unnecessary dates\n",
    "min_holdout_forecast_date = get_min_test_date(holdout_dates_final)\n",
    "holdout_forecast_cut = holdout_forecast[(holdout_forecast.ds >= min_holdout_forecast_date)]\n",
    "holdout_forecast_cut.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns in order to further filter out (see explanation in test set).\n",
    "holdout_forecast_cut['month_cutoff'] = holdout_forecast_cut['cutoff'].apply(lambda x: x.month)\n",
    "holdout_forecast_cut['year_cutoff'] = holdout_forecast_cut['cutoff'].apply(lambda x: x.year)\n",
    "holdout_forecast_cut['month_ds'] = holdout_forecast_cut['ds'].apply(lambda x: x.month)\n",
    "holdout_forecast_cut['year_ds'] = holdout_forecast_cut['ds'].apply(lambda x: x.year)\n",
    "holdout_forecast_cut = holdout_forecast_cut[(holdout_forecast_cut.month_cutoff == holdout_forecast_cut.month_ds) & \\\n",
    "                                           (holdout_forecast_cut.year_cutoff == holdout_forecast_cut.year_ds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions using samples to estimate errors for aggregate forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregated predictions\n",
    "all_rolled_preds = get_aggregate_forecast_error(ho_m, ho_future, holdout_dates_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rolled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregates for real values as well\n",
    "all_real = holdout_forecast_cut[(holdout_forecast_cut.ds <= holdout_forecast_cut.cutoff) & \\\n",
    "                               ((holdout_forecast_cut.month_cutoff == holdout_forecast_cut.month_ds) & \\\n",
    "                                           (holdout_forecast_cut.year_cutoff == holdout_forecast_cut.year_ds))][['ds', 'y', 'cutoff']]\n",
    "all_real['prediction_day'] = all_real['cutoff'].apply(lambda x: (x + timedelta(days=1)).date())\n",
    "all_rolled_real = all_real.groupby('prediction_day').sum().reset_index()\n",
    "all_rolled_real['prediction_day'] = all_rolled_real.prediction_day.astype(str)\n",
    "#all_rolled_real\n",
    "all_rolled_real.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge aggregated real and forecasted values\n",
    "holdout_aggr_forecasts = all_rolled_preds.merge(all_rolled_real, on=\"prediction_day\", how='outer')\n",
    "holdout_aggr_forecasts['monthly_forecast'] = holdout_aggr_forecasts['y'] + holdout_aggr_forecasts['yhat']\n",
    "holdout_aggr_forecasts = holdout_aggr_forecasts[['prediction_day', 'monthly_forecast', 'yhat_lower', 'yhat_upper']]\n",
    "holdout_aggr_forecasts.columns = ['prediction_day', 'monthly_forecast', 'monthly_forecast_lower', 'monthly_forecast_upper']\n",
    "holdout_aggr_forecasts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "holdout_aggr_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
